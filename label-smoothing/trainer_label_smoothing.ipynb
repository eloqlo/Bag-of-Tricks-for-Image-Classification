{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CrossEntropy](./CrossEntropyLoss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_enable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jh/anaconda3/envs/test_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_archive.ResNet import ResNet, Config\n",
    "\n",
    "import argparse\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    model='resnet50',\n",
    "    batch_size=16,\n",
    "    lr=1.25e-4,\n",
    "    epochs=100,\n",
    "    root_path='/home/jh/Desktop/VSC/CNN_work/archive/CUB_200_2011/',\n",
    "    scheduler='no',\n",
    "    pretrained='no',\n",
    "\n",
    "    cosLRdecay_lin_end=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def calculate_topk_accuracy(y_pred, y, k = 5):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(k, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim = True)\n",
    "        acc_1 = correct_1 / batch_size\n",
    "        acc_k = correct_k / batch_size\n",
    "    return acc_1, acc_k\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step, (x, y) in enumerate(iterator):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler=='yes':\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc_1 += acc_1.item()\n",
    "        epoch_acc_5 += acc_5.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc_1 += acc_1.item()\n",
    "            epoch_acc_5 += acc_5.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing CEL !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Dataset\n",
    "    pretrained_size = 224\n",
    "    pretrained_means = [0.485, 0.456, 0.406]\n",
    "    pretrained_stds= [0.229, 0.224, 0.225]\n",
    "    train_transforms = transforms.Compose([\n",
    "                            transforms.Resize(pretrained_size),\n",
    "                            transforms.RandomRotation(5),\n",
    "                            transforms.RandomHorizontalFlip(0.5),\n",
    "                            transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean = pretrained_means, \n",
    "                                                    std = pretrained_stds)\n",
    "                        ])\n",
    "    test_transforms = transforms.Compose([\n",
    "                            transforms.Resize(pretrained_size),\n",
    "                            transforms.CenterCrop(pretrained_size),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean = pretrained_means, \n",
    "                                                    std = pretrained_stds)\n",
    "                        ])\n",
    "    \n",
    "    ROOT = args.root_path\n",
    "    data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "    train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                  transform = train_transforms)\n",
    "    test_data = datasets.ImageFolder(root = test_dir,\n",
    "                                    transform = test_transforms)\n",
    "    \n",
    "    VALID_RATIO = 0.8\n",
    "    n_train_examples = int(len(train_data)*VALID_RATIO)\n",
    "    n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "    train_data, valid_data = data.random_split(train_data,\n",
    "                                            [n_train_examples, n_valid_examples])\n",
    "    valid_data = copy.deepcopy(valid_data)\n",
    "    valid_data.dataset.transform = test_transforms\n",
    "\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    train_iterator = data.DataLoader(train_data, \n",
    "                                    shuffle = True, \n",
    "                                    batch_size = BATCH_SIZE)\n",
    "    valid_iterator = data.DataLoader(valid_data, \n",
    "                                    batch_size = BATCH_SIZE)\n",
    "    test_iterator = data.DataLoader(test_data, \n",
    "                                    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        \"\"\"\n",
    "        classes : 분류하려는 클래스 수\n",
    "        smoothing : 감도\n",
    "        dim : pred에서 softmax하려는 dimension\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)   # \n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9347, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9347, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss_ce = nn.CrossEntropyLoss()\n",
    "loss_ls = LabelSmoothingLoss(classes=len(train_iterator), smoothing=0.0, dim=-1)\n",
    "\n",
    "input = torch.randn(3,5,requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "# print(loss_ce(input, target))\n",
    "print(loss_ls(input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<< Configurations >>\n",
      "[*] Model       - resnet50\n",
      "[*] Batch_size  - 64\n",
      "[*] LR          - 5e-05\n",
      "[*] Epochs      - 100\n",
      "[*] train newly initialized model!\n",
      "[*] Parameters  - 23,917,832\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--model\", type=str, default='resnet34')     # resnet18, 34, 50, 101, 152.\n",
    "    # parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    # parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    # parser.add_argument(\"--root_path\", type=str, default='/home/jh/Desktop/VSC/CNN_work/archive/CUB_200_2011/')\n",
    "    # parser.add_argument(\"--scheduler\", type=str, default='no')      # yes / no\n",
    "    # parser.add_argument(\"--pretrained\", type=str, default='no')     # yes / no\n",
    "    # # parser.add_argument(\"--half\", type=str, default='no')           # yes / no\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    print()\n",
    "    print('<< Configurations >>')\n",
    "    print(f'[*] Model       - {args.model}')\n",
    "    print(f'[*] Batch_size  - {args.batch_size}')\n",
    "    print(f'[*] LR          - {args.lr}')\n",
    "    print(f'[*] Epochs      - {args.epochs}')\n",
    "\n",
    "    # Dataset\n",
    "    pretrained_size = 224\n",
    "    pretrained_means = [0.485, 0.456, 0.406]\n",
    "    pretrained_stds= [0.229, 0.224, 0.225]\n",
    "    train_transforms = transforms.Compose([\n",
    "                            transforms.Resize(pretrained_size),\n",
    "                            transforms.RandomRotation(5),\n",
    "                            transforms.RandomHorizontalFlip(0.5),\n",
    "                            transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean = pretrained_means, \n",
    "                                                    std = pretrained_stds)\n",
    "                        ])\n",
    "    test_transforms = transforms.Compose([\n",
    "                            transforms.Resize(pretrained_size),\n",
    "                            transforms.CenterCrop(pretrained_size),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean = pretrained_means, \n",
    "                                                    std = pretrained_stds)\n",
    "                        ])\n",
    "    \n",
    "    ROOT = args.root_path\n",
    "    data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "    train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                  transform = train_transforms)\n",
    "    test_data = datasets.ImageFolder(root = test_dir,\n",
    "                                    transform = test_transforms)\n",
    "    \n",
    "    VALID_RATIO = 0.8\n",
    "    n_train_examples = int(len(train_data)*VALID_RATIO)\n",
    "    n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "    train_data, valid_data = data.random_split(train_data,\n",
    "                                            [n_train_examples, n_valid_examples])\n",
    "    valid_data = copy.deepcopy(valid_data)\n",
    "    valid_data.dataset.transform = test_transforms\n",
    "\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    train_iterator = data.DataLoader(train_data, \n",
    "                                    shuffle = True, \n",
    "                                    batch_size = BATCH_SIZE)\n",
    "    valid_iterator = data.DataLoader(valid_data, \n",
    "                                    batch_size = BATCH_SIZE)\n",
    "    test_iterator = data.DataLoader(test_data, \n",
    "                                    batch_size = BATCH_SIZE)\n",
    "\n",
    "    # Model Zoo\n",
    "    if args.model=='resnet18':\n",
    "        if args.pretrained=='yes':\n",
    "            downloaded_model = models.resnet18(pretrained = True)\n",
    "            print('[*] pre-trained model being used!')\n",
    "        else:\n",
    "            downloaded_model = models.resnet18(pretrained = False)\n",
    "            print('[*] train newly initialized model!')\n",
    "    elif args.model=='resnet34':\n",
    "        if args.pretrained=='yes':\n",
    "            downloaded_model = models.resnet34(pretrained = True)\n",
    "            print('[*] pre-trained model being used!')\n",
    "        else:\n",
    "            downloaded_model = models.resnet34(pretrained = False)\n",
    "            print('[*] train newly initialized model!')\n",
    "    elif args.model=='resnet50':\n",
    "        if args.pretrained=='yes':\n",
    "            downloaded_model = models.resnet50(pretrained = True)\n",
    "            print('[*] pre-trained model being used!')\n",
    "        else:\n",
    "            downloaded_model = models.resnet50(pretrained = False)\n",
    "            print('[*] train newly initialized model!')\n",
    "    elif args.model=='resnet101':\n",
    "        if args.pretrained=='yes':\n",
    "            downloaded_model = models.resnet101(pretrained = True)\n",
    "            print('[*] pre-trained model being used!')\n",
    "        else:\n",
    "            downloaded_model = models.resnet101(pretrained = False)\n",
    "            print('[*] train newly initialized model!')\n",
    "    elif args.model=='resnet152':\n",
    "        if args.pretrained=='yes':\n",
    "            downloaded_model = models.resnet152(pretrained = True)\n",
    "            print('[*] pre-trained model being used!')\n",
    "        else:\n",
    "            downloaded_model = models.resnet152(pretrained = False)\n",
    "            print('[*] train newly initialized model!')\n",
    "    config = Config()\n",
    "    resnet_config = config.get_resnet_config(model_name = args.model)\n",
    "\n",
    "\n",
    "    # Change FC layer in model for transfer learning.\n",
    "    IN_FEATURES = downloaded_model.fc.in_features \n",
    "    OUTPUT_DIM = len(test_data.classes)\n",
    "    downloaded_model.fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n",
    "\n",
    "    START_LR = args.lr\n",
    "    model = ResNet(resnet_config, OUTPUT_DIM)\n",
    "    print(f'[*] Parameters  - {count_parameters(model):,}')\n",
    "    # if args.half=='yes':\n",
    "    #     model = model.half()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=START_LR)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "    # ! What I am dealing with! # ! # ! # ! # ! # ! # ! # ! # ! \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = LabelSmoothingLoss(len(train_iterator))\n",
    "    # ! # ! # ! # ! # ! # ! # ! # ! # ! # ! # ! # ! # ! # ! # !\n",
    "\n",
    "\n",
    "    if args.scheduler == 'yes':\n",
    "        # cosine scheduler\n",
    "        '''\n",
    "        - 기존 lr overfitting지점인, 30-40 에 다다르기 전에 decay주는게 적합해보여.\n",
    "        - 한번 그렇게 ``lr==0`` 까지 탐색하는것보단, hard_reset하면서 그 optima에서 빠져나와서 \n",
    "          주변 다른 optima 들어가보는것도 좋지 않을까?\n",
    "        - 지금 실험상황은 best 모델 찾는거고, epoch 100 가면서 어차피 튀는 경향성 보이니, 좋은 시도같은데?\n",
    "        '''\n",
    "        ITERATIONS = args.epochs * len(train_iterator)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=ITERATIONS, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler=None\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if tensorboard_enable:\n",
    "        writer = SummaryWriter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training.\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_epoch = 0\n",
    "\n",
    "print('[*] Start Training !', end='\\n\\n')\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer, criterion, device, scheduler)\n",
    "    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if tensorboard_enable:\n",
    "        writer.add_scalar(\"loss/train\", train_loss, epoch)  # tensorboard\n",
    "        writer.add_scalar(\"loss/val\", valid_loss, epoch)    # tensorboard\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_valid_epoch = epoch\n",
    "        torch.save(model.state_dict(), f'./saved/{args.model}_bs{args.batch_size}_lr{args.lr}_epochs{args.epochs}_pretrained-{args.pretrained}.pt')\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ' \\\n",
    "        f'Train Acc @5: {train_acc_5*100:6.2f}%')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1*100:6.2f}% | ' \\\n",
    "        f'Valid Acc @5: {valid_acc_5*100:6.2f}%')\n",
    "\n",
    "print()\n",
    "print(f\"Best valid epoch : {best_valid_epoch}/{args.epochs} epochs\")\n",
    "\n",
    "if tensorboard_enbale:\n",
    "    writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
