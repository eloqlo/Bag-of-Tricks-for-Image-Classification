{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_tensorboard = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace(\n",
    "    model='resnet50',\n",
    "    batch_size=128,\n",
    "    lr=1.25e-4,\n",
    "    epochs=100,\n",
    "    root_path='/root/datasets/archive/CUB_200_2011/',\n",
    "    scheduler='no',\n",
    "    pretrained='yes',\n",
    "    teacher_path='/root/workspace/CNN_work/teacher_model/teacher_model.pt'\n",
    ")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model\", type=str, default='resnet34')     # resnet18, 34, 50, 101, 152.\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "# parser.add_argument(\"--root_path\", type=str, default='/home/jh/Desktop/VSC/CNN_work/archive/CUB_200_2011/')\n",
    "# parser.add_argument(\"--scheduler\", type=str, default='no')      # yes / no\n",
    "# parser.add_argument(\"--pretrained\", type=str, default='no')     # yes / no\n",
    "# # parser.add_argument(\"--half\", type=str, default='no')           # yes / no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/test_jh/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_archive.ResNet import ResNet, Config\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def calculate_topk_accuracy(y_pred, y, k = 5):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(k, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim = True)\n",
    "        acc_1 = correct_1 / batch_size\n",
    "        acc_k = correct_k / batch_size\n",
    "    return acc_1, acc_k\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler=='yes':\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc_1 += acc_1.item()\n",
    "        epoch_acc_5 += acc_5.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc_1 += acc_1.item()\n",
    "            epoch_acc_5 += acc_5.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model = models.resnet152(pretrained = True)\n",
    "for c in model.parameters():\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model\", type=str, default='resnet34')     # resnet18, 34, 50, 101, 152.\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "# parser.add_argument(\"--root_path\", type=str, default='/home/jh/Desktop/VSC/CNN_work/archive/CUB_200_2011/')\n",
    "# parser.add_argument(\"--scheduler\", type=str, default='no')      # yes / no\n",
    "# parser.add_argument(\"--pretrained\", type=str, default='no')     # yes / no\n",
    "# # parser.add_argument(\"--half\", type=str, default='no')           # yes / no\n",
    "# args = parser.parse_args()\n",
    "\n",
    "print()\n",
    "print('<< Configurations >>')\n",
    "print(f'[*] Model       - {args.model}')\n",
    "print(f'[*] Batch_size  - {args.batch_size}')\n",
    "print(f'[*] LR          - {args.lr}')\n",
    "print(f'[*] Epochs      - {args.epochs}')\n",
    "\n",
    "# Dataset\n",
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "train_transforms = transforms.Compose([\n",
    "                        transforms.Resize(pretrained_size),\n",
    "                        transforms.RandomRotation(5),\n",
    "                        transforms.RandomHorizontalFlip(0.5),\n",
    "                        transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                    ])\n",
    "test_transforms = transforms.Compose([\n",
    "                        transforms.Resize(pretrained_size),\n",
    "                        transforms.CenterCrop(pretrained_size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                    ])\n",
    "ROOT = args.root_path\n",
    "data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                transform = train_transforms)\n",
    "test_data = datasets.ImageFolder(root = test_dir,\n",
    "                                transform = test_transforms)\n",
    "\n",
    "VALID_RATIO = 0.8\n",
    "n_train_examples = int(len(train_data)*VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data,\n",
    "                                        [n_train_examples, n_valid_examples])\n",
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                shuffle = True, \n",
    "                                batch_size = BATCH_SIZE)\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                batch_size = BATCH_SIZE)\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "# get pretrained model.\n",
    "if args.model=='resnet18':\n",
    "    if args.pretrained=='yes':\n",
    "        model = models.resnet18(pretrained = True)\n",
    "        print('[*] pre-trained model being used!')\n",
    "elif args.model=='resnet34':\n",
    "    if args.pretrained=='yes':\n",
    "        model = models.resnet34(pretrained = True)\n",
    "        print('[*] pre-trained model being used!')\n",
    "elif args.model=='resnet50':\n",
    "    if args.pretrained=='yes':\n",
    "        model = models.resnet50(pretrained = True)\n",
    "        print('[*] pre-trained model being used!')\n",
    "elif args.model=='resnet101':\n",
    "    if args.pretrained=='yes':\n",
    "        model = models.resnet101(pretrained = True)\n",
    "        print('[*] pre-trained model being used!')\n",
    "elif args.model=='resnet152':\n",
    "    if args.pretrained=='yes':\n",
    "        model = models.resnet152(pretrained = True)\n",
    "        print('[*] pre-trained model being used!')\n",
    "\n",
    "if args.pretrained!='yes':\n",
    "    # make new model.\n",
    "    print('[*] train newly initialized model!')\n",
    "    config = Config()\n",
    "    resnet_config = config.get_resnet_config(model_name = args.model)\n",
    "    OUTPUT_DIM = len(test_data.classes)\n",
    "    model = ResNet(resnet_config, OUTPUT_DIM)  # get resnetXXX\n",
    "\n",
    "    print(f'[*] Parameters  - {count_parameters(model):,}')\n",
    "else:\n",
    "    # Change FC layer in downloaded model for Transfer Learning.\n",
    "    IN_FEATURES = model.fc.in_features \n",
    "    OUTPUT_DIM = len(test_data.classes)\n",
    "    model.fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n",
    "\n",
    "    print(f'[*] Parameters  - {count_parameters(model):,}')\n",
    "\n",
    "# < low-precision training >\n",
    "# if args.half=='yes':\n",
    "#     model = model.half()\n",
    "\n",
    "START_LR = args.lr\n",
    "\n",
    "#* discriminative fine-tuning\n",
    "#? How do I know my model's hierarchical information?\n",
    "params = [\n",
    "    {'params': model.conv1.parameters(), 'lr': START_LR / 10},\n",
    "    {'params': model.bn1.parameters(), 'lr': START_LR / 10},\n",
    "    {'params': model.layer1.parameters(), 'lr': START_LR / 8},\n",
    "    {'params': model.layer2.parameters(), 'lr': START_LR / 6},\n",
    "    {'params': model.layer3.parameters(), 'lr': START_LR / 4},\n",
    "    {'params': model.layer4.parameters(), 'lr': START_LR / 2},\n",
    "    {'params': model.fc.parameters()}\n",
    "    ]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=START_LR) #? How does this work?, Does this work on any other optimizers?\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# * Use StepLR Scheduler\n",
    "if args.lin_scheduler=='yes':\n",
    "    print('[*] Using StepLR Scheduler - step_size=30, gamma=0.5')\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "else:\n",
    "    scheduler=None\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "if enable_tensorboard:\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "# Model training.\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_epoch = 0\n",
    "\n",
    "print('[*] Start Training !', end='\\n\\n')\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer, criterion, device, scheduler)\n",
    "    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if enable_tensorboard:\n",
    "        writer.add_scalar(\"loss/train\", train_loss, epoch)  # tensorboard\n",
    "        writer.add_scalar(\"loss/val\", valid_loss, epoch)    # tensorboard\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_valid_epoch = epoch\n",
    "        torch.save(model.state_dict(), f'./saved/{args.model}_bs{args.batch_size}_lr{args.lr}_epochs{args.epochs}_pretrained-{args.pretrained}.pt')\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ' \\\n",
    "        f'Train Acc @5: {train_acc_5*100:6.2f}%')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1*100:6.2f}% | ' \\\n",
    "        f'Valid Acc @5: {valid_acc_5*100:6.2f}%')\n",
    "\n",
    "print()\n",
    "print(f\"Best valid epoch : {best_valid_epoch}/{args.epochs} epochs\")\n",
    "\n",
    "if enable_tensorboard:\n",
    "    writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_jh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
